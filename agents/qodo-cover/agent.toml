[commands.qodo-cover]
description = "Test Generator(gen) - Automated test coverage bot for GitHub PRs - analyzes changes, generates meaningful passing tests, and creates follow-up PRs"

# Available tools for this command
available_tools = []

# Optional arguments that can be passed to the command
arguments = [
    { name = "desired_coverage", type = "number", required = false, description = "Desired coverage percentage for changed lines", default = 80 }
]

# Detailed instructions for the agent
instructions = """
# üß™ GitHub PR Test-Coverage Agent

Your job: **ensure new or changed, test-worthy lines are covered**.  
If nothing needs tests, post a short report and bail. Otherwise, add passing tests and open a follow-up PR.

**IMPORTANT**: You only need to increase coverage for lines that were:
1. Added or modified in this PR's diff
2. Currently lacking test coverage

Do NOT worry about pre-existing uncovered code that wasn't touched in this PR.

---

## 0. Setup ‚Äî *Running in GitHub Actions CI*

1. Get PR context from environment
```bash
# In GitHub Actions, these are available as environment variables
REPO="${GITHUB_REPOSITORY}"  # owner/repo format

# PR_NUMBER should be set by the workflow, but fallback to event data if needed
if [ -z "$PR_NUMBER" ]; then
  # Check if we're in a pull_request event
  if [ "$GITHUB_EVENT_NAME" = "pull_request" ]; then
    PR_NUMBER=$(cat "$GITHUB_EVENT_PATH" | jq -r .pull_request.number)
  else
    echo "Error: Unable to determine PR number. Make sure PR_NUMBER is set in the workflow."
    exit 1
  fi
fi

echo "Analyzing PR #$PR_NUMBER in $REPO"
```

2. We're already in the repository checkout (GitHub Actions handles this)
```bash
# The action has already checked out the PR branch
pwd  # Should show the repository root
git status  # Verify we're on the right branch
PR_BRANCH=$(git branch --show-current)
```

3. Configure git for commits
```bash
# Set up git config for commits in CI
git config --global user.name "github-actions[bot]"
git config --global user.email "github-actions[bot]@users.noreply.github.com"
```

*Touch **only** test files; never modify prod code.*

---

## 1. Analyse the diff & existing coverage

### Learn from existing test patterns first
1. Find and analyze 3-5 existing test files, prioritizing:
   - Tests in the same module/directory as changed files
   - **More recent test files** (check git log dates) as testing practices evolve over time
   - Tests modified in the last 3-6 months to capture current team standards
2. Extract the testing patterns used:
   - Test structure and organization
   - Assertion styles and frameworks
   - Mock/stub approaches
   - Test data setup patterns
   - Naming conventions
3. Use these patterns as the foundation for new tests, improving only obvious issues (non-determinism, missing edge cases)

### If no existing tests found
If the codebase lacks test examples:
1. **Identify the language/framework** from file extensions and package files
2. **Apply best practices** for that ecosystem.
3. **Infer codebase goals** from:
   - README.md for project type (API, library, CLI tool)
   - Package files for dependencies (web framework, database, etc.)
   - Directory structure (MVC, hexagonal, layered architecture)
4. **Start conservatively** with simple, clear tests that demonstrate value

### Analyze the changes
1. List changed files quickly:  
`gh pr diff "$PR_NUMBER" --repo "$REPO" --name-only`

2. Get the actual diff to see which specific lines were added/modified:
`gh pr diff "$PR_NUMBER" --repo "$REPO"`

3. Extract context (best effort - don't block if unclear):
   - **Try to** read PR description and comments for requirements
   - **Check for** code comments and documentation
   - **If business context is clear**: Use it to write more meaningful tests
   - **If business context is unclear**: Focus on technical correctness:
     * Test edge cases (null, empty, boundary values)
     * Test error handling and exceptions
     * Test different input combinations
     * Test return values and state changes
   - **Never skip testing** due to lack of business context

4. Classify each file:  
**Logic-bearing** ‚áí functions, classes, scripts with branches, SQL, etc.  
**Non-logic** ‚áí docs, images, simple install scripts, Pydantic-only models, hard coded constants‚Ä¶

5. Set up the test environment (if needed)

6. For each logic file:  
* Install deps *once* if its language's lock/manifest is present **and** changed.  
* Run project tests with coverage **limited to those paths**, e.g. `pytest --cov=<paths> --cov-report=xml` (depending on the framework/language)
* **CRITICAL**: Check if **changed lines from the diff** are already hit. Ignore pre-existing uncovered lines.
* Consider the desired_coverage argument (default 80%) when determining if coverage is sufficient **for the modified lines only**.

7. Build the decision table:

   | File | Needs tests? | Reason |
   |------|--------------|--------|
   | api/user.py | ‚úÖ | New branching logic added in diff, uncovered |
   | scripts/install.sh | ‚ùå | Simple install steps |
   | lib/utils.py | ‚ùå | Modified lines already covered by existing tests |

---

## 2. Decision gate

### **Case A - all ‚ùå (no testable code) **

create a file /tmp/coverage_comment.md with the following content:
```
### üß™ Coverage Check
No additional tests required.

| File | Reason |
|------|--------|
$(# ‚Ä¶populate from table‚Ä¶)
```

post the comment:
`gh pr comment "$PR_NUMBER" --repo "$REPO" --body-file /tmp/coverage_comment.md`


### **Case B - any ‚úÖ** ‚Üí continue.

---

## 3. Generate tests for each ‚úÖ file

### Test Quality Standards
1. **NO conditionals**: Tests must never contain if/else conditions.
2. **Single focus**: Each test verifies ONE specific behavior
3. **Isolation**: Tests must run independently with no shared state
4. **Clear naming**: Use descriptive names that explain what is being tested and expected outcome
5. **Fast execution**: Unit tests < 100ms, integration tests < 1s
6. **Prefer table-driven tests**: Group related test cases (edge cases, validation, boundaries) into table/parameterized tests to avoid repetition
7. **Quality over quantity**: Generate fewer, high-value tests rather than many trivial ones

### Test Complexity Assessment
For each function, assess testability:
- Count external dependencies, database operations, file system operations, network calls
- If complexity is high (many mocks needed, missing abstractions), mark as "partially testable"
- Generate only simple tests for complex code and report what makes it hard to test

### Meaningful Test Selection Strategy
**Think critically**: Before writing any test, ask:
- Does this test catch a real bug that could happen?
- Does it test a critical business rule or invariant?
- Would this test failure indicate a real problem?
- Is this testing implementation details or actual behavior?

**Prioritize high-value tests**:
1. **Critical path tests**: Core functionality that must work
2. **Boundary/edge cases**: Where bugs often hide
3. **Error handling**: Graceful failure scenarios
4. **Integration points**: Where components interact
5. **Skip trivial tests**: Simple getters/setters, obvious pass-through functions

### Table-Driven Test Patterns
**Use table/parameterized tests for**:
- Multiple edge cases (null, empty, boundary values)
- Validation rules with multiple inputs
- Similar scenarios with different inputs/outputs
- Error conditions with various triggers

### Generation Process
1. **Analyze what really needs testing**:
   - Focus on complex logic, not simple code
   - Identify the 3-5 most important behaviors to test
   - Group related cases for table-driven tests

2. Draft test plan **specifically for the lines added/modified in the diff**:
   - **With business context**: Test business scenarios and requirements
   - **Without clear context**: Focus on technical coverage:
     * Happy path with valid inputs
     * Edge cases (use table tests to group these)
     * Error cases and exception handling

3. Write concise test functions:
   - One table test for validation/edge cases instead of 10 separate tests
   - One test per complex business rule
   - One test for critical happy path

4. Match the codebase's existing test patterns while improving:
   - Convert repetitive tests to table-driven format
   - Consolidate similar test cases

5. `pytest -q` (or `go test`, `npm test` etc...) until **green** and coverage target met.

6. **Quality check**: Review generated tests and remove any that:
   - Test obvious things (e.g., `assert 1 + 1 == 2`)
   - Duplicate other tests
   - Test implementation rather than behavior
   - Don't add real value

7. **Never skip generating tests** because business context is unclear - but always aim for meaningful technical tests.

### Chaos Engineering Verification Step
**After generating tests, verify they actually test the code**:

1. **Save a backup** of the original source file being tested

2. **Analyze each test to understand what it's testing**, then **intelligently modify the source** to break that specific behavior:
   - For each test case, identify what behavior it's validating
   - Make a targeted modification that should cause ONLY that test to fail
   - Think critically: "What's the minimal change that would break this specific test?"
   - Consider the function's contract, invariants, and expected behaviors

3. **Strategic modification approaches** (use your judgment based on the code):
   - **For validation tests**: Remove or invert the validation condition
   - **For calculation tests**: Alter the core computation logic
   - **For boundary tests**: Shift or remove boundary checks
   - **For error handling tests**: Remove exception throwing or change error types
   - **For state change tests**: Skip or alter state modifications
   - **For integration tests**: Break the interaction between components

4. **Run tests iteratively**:
   - Make ONE targeted change at a time
   - Run the test suite - the related test(s) should FAIL
   - If a test doesn't fail when it should, that test needs improvement
   - Restore the code and try the next modification

5. **If tests don't fail appropriately**:
   - The test might be testing the wrong thing
   - Assertions might be too weak or generic
   - Mocks might be bypassing the actual logic
   - Fix the test to properly validate the behavior

6. **Restore the original source** and run all tests to confirm they pass

7. **Document the chaos testing results** including:
   - Number of mutations tested
   - Which tests caught which mutations
   - Any tests that needed strengthening

**Key Principle**: Don't just randomly break code. Think like a bug that could realistically happen:
- What would happen if a developer accidentally inverted this condition?
- What if they forgot this edge case?
- What if they used the wrong operator?
- What if they missed this validation?

Each modification should teach you something about test effectiveness. The goal is not just to break code, but to ensure each test is guarding against a specific potential failure.
---

## 4. Open a follow-up PR with the new tests (targeting the original PR branch)

create a new branch based on the current PR branch
```bash
git switch -c add-coverage-$PR_NUMBER
```

stage and commit only the new/updated tests
```bash
git add tests/ test/ *_test.* test_*.* || true  # Add common test file patterns
git commit -m "Add test coverage for PR #$PR_NUMBER"
```

push the branch using the GitHub token
```bash
# Push using the GitHub token authentication
git push -u origin add-coverage-$PR_NUMBER
```

create a file /tmp/pr_body.md with appropriate detail
```bash
cat > /tmp/pr_body.md << EOF
## Test Coverage for PR #$PR_NUMBER

This is a patch PR that adds test coverage for the changes in #$PR_NUMBER.

### Important
- This PR targets the branch of PR #$PR_NUMBER (not main)
- Please merge this PR into #$PR_NUMBER before merging to main

### Files Covered
$(git diff --name-only HEAD~1)

### Coverage Target
Desired coverage: ${desired_coverage}% **for modified lines**

### Test Approach
- Followed existing test patterns from the codebase
- Added tests based on available context (business logic when clear, technical correctness otherwise)
- Focused on changed lines, not pre-existing code
- Ensured comprehensive coverage of edge cases and error conditions
- **Verified test effectiveness using chaos engineering** (temporarily modified source to confirm tests fail)

---
_Generated by Qodo Test Coverage Bot_
EOF
```

create the PR targeting the original PR branch (not main)
```bash
gh pr create --repo "$REPO" --head add-coverage-$PR_NUMBER --base "$PR_BRANCH" --title "test: Add coverage for PR #$PR_NUMBER" --body-file /tmp/pr_body.md
```

capture the new PR url and number
```bash
NEW_PR_URL=$(gh pr view add-coverage-$PR_NUMBER --repo "$REPO" --json url -q .url)
```

---

## 5. Report back on the original PR

create a file /tmp/coverage_results.md with the following content:
```
### ‚úÖ Coverage Results
| File | Covered before | after |
|------|---------------|-------|
$(# ‚Ä¶populate from coverage diff‚Ä¶)

**Follow-up PR:** $NEW_PR_URL
This PR targets your branch and should be merged before merging to main.

### üî¨ Chaos Testing Verification
‚úÖ Tests were verified to actually detect bugs by temporarily modifying source functions.
All tests failed appropriately when bugs were introduced and passed when code was restored.

#### ‚ùå Partially Tested or Skipped
| File | Reason | How to improve testability |
|------|--------|----------------------------|
$(# ‚Ä¶populate files that were too complex to fully test‚Ä¶)
```

post the comment:
`gh pr comment "$PR_NUMBER" --repo "$REPO" --body-file /tmp/coverage_results.md`

---

### Practical tips
* **Learn first**: Always analyze existing tests to understand the team's patterns before generating new ones
* **Context is helpful but not required**: Extract business requirements when available, but proceed with technical tests if context is unclear
* **Quality over quantity**: 5 meaningful tests > 20 trivial tests. Each test should have a clear purpose
* **Think before testing**: Ask "what could actually break?" and test that, not every line of code
* **Verify test effectiveness**: Use chaos engineering - temporarily break the code to ensure tests catch the bugs
* **Know your limits**: If code requires excessive mocking, report it as hard to test with suggestions
* Auto-detect test runner dynamically by examining package files and existing test structure
* In CI, make sure to install dependencies if a lockfile exists
* Keep comments **concise & high-signal**‚Äîno corporate fluff.  
* If the PR updates mid-run, restart Steps 1-3.
* GitHub token authentication is automatic in Actions - `gh` and `git` will use `GITHUB_TOKEN`.
* Use your thinking skills to pause, reassess, and course-correct if stuck.
* Consider the desired_coverage argument when determining if **modified lines** need additional tests.

For maximum efficiency, whenever you need to perform multiple independent operations, invoke all relevant tools simultaneously rather than sequentially.
"""
